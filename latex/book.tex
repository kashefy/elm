%% Based on a TeXnicCenter-Template by Gyorgy SZEIDL.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------
%
\documentclass{report}%
%
%----------------------------------------------------------
% This is a sample document for the standard LaTeX Book Class
% Class options
%       --  Body text point size:
%                        10pt (default), 11pt, 12pt
%       --  Paper size:  letterpaper (8.5x11 inch, default)
%                        a4paper, a5paper, b5paper,
%                        legalpaper, executivepaper
%       --  Orientation (portrait is the default):
%                        landscape
%       --  Printside:   oneside, twoside (default)
%       --  Quality:     final(default), draft
%       --  Title page:  titlepage, notitlepage
%       --  Columns:     onecolumn (default), twocolumn
%       --  Start chapter on left:
%                        openright(no, default), openany
%       --  Equation numbering (equation numbers on right is the default):
%                        leqno
%       --  Displayed equations (centered is the default):
%                        fleqn (flush left)
%       --  Open bibliography style (closed bibliography is the default):
%                        openbib
% For instance the command
%          \documentclass[a4paper,12pt,reqno]{book}
% ensures that the paper size is a4, fonts are typeset at the size 12p
% and the equation numbers are on the right side.
%
\usepackage{amsmath}
\usepackage{hyperref}
\begin{document}

%\frontmatter
\title{Scale- and Translation-Invariant Unsupervised Learning of Hidden Causes Using Spiking Neurons with Top-Down Attention}
\author{Youssef Kashef}
\date{7. August 2013}
\maketitle
\tableofcontents

\chapter{Abstract}

Nessler et al have demonstrated the ability of a spiking neuronal network governed by spike-timing-dependent-plasticity and a stochastic winner-take-all circuit to learn and predict causes from visual input. We aim to increase the computational power of the existing network through invariance to translation and scale. The visual system of the brain masters the recognition of objects wherever they appear in the visual scene and regardless of scale, orientation or even with partial occlusions. It achieves this through attention. Therefore, we turn to the pool of literature on modeling visual attention systems inspired from the brain. The architecture of the extended model is composed of the existing recognition module whose response modulates the attention module to be constructed in a top-down manner. This modulation will allow the attention module to alter the input window exposed for recognition. Attention is modeled as a network measuring for saliency in a scene by feature extraction with the use of hierarchies. The design and development of this extended model to achieve the required invariance using processes that approximate their biological counterparts is presented. Emphasis is put on making these approximations through computationally economic implementations. Evaluation of the model is based on its performance in a set of experiments as well as its computational efficiency. Experiments are constructed to scrutinize the behavior of the model, its ability to converge onto a sight within a scene that enables recognition. Artificial as well as natural images are used to further reveal the capabilities and limitations of our approach.

\chapter{Introduction}

elaborated abstract with references.......\cite{Nessler2010}

%\mainmatter

\chapter{Object Recognition with Spike Expectation Maximization}

\section{Spike Expectation Maximization}

Literature review of existing SEM model

Nessler et. al have articulated a bayesian model of how the brain analyszes sensory stimuli. The model demostrates the learning of hidden causes in visual stimuli emerging through by identifying correlations of a winner-take-all (WTA) network of spiking neurons that are activated continuously in the presence of their preferred stimulus. The model demonstrates utilizing Spike Time Dependent Plasticity (STDP) in WTA circuits as an approximation of Expectation Maximization. 

\section{SEM for learning features}

\subsection{Extending SEM by learning orientations}

The current encoding of external variables accounts for the intensities of the spatial units (pixels) of a presented stimulus. The encoding of intensities is performed through a population coding by antognistic binary nodes per pixel that drive a poisson process \cite{Nessler2010}. Prallel to these intensity encoded nodes, we add a WTA circuit per pixel that determines the preferred orientation of this node relative to its spatial neighbors. This creates an orientation map of the presented stimulus. Whilst counter-intuitive with traditional learning models, SEM benefits from increasing the dimensionality of its feature space as this increases its resolution for detecting correlations between an output node $z$ and input nodes $y$ on a linear scale. Recalling the use of using population coding to encode in atagonistic (on-node, off-node) fashion, thus letting the WTA learn the likelihood of an input node firing, or not firing, explicitly, as shown by \ref{eqn_corr}.

\begin{equation}
	p(z=1|y) \propto y*p(y=1|z) + (1-y)*p(y=0|z)
	\label{eqn_corr}
\end{equation}

As we introduce the orientation map we may add additional operands to \ref{eqn_corr} to account for the nodes preferred orientation.

\begin{equation}
	\begin{split}
		p(z=1|y) \propto &\frac{y_I*p(y_I=1|z) + (1-y_I)*p(y_I=0|z)}{N_o-2} \\
			&+ \frac{a}{N_o}[y_o*p(y_O=O_i)+(N_O-1)*\sum_{i\neq o_i}p(y_O=o_i)]\\
	\end{split}
	\label{eqn_corr2}
\end{equation}

where
\begin{itemize}
  \item $y_I$ denotes an intensity node,
  \item $y_I$ denotes an intensity node,
  \item $N_O$ denotes the number of orientations available,
  \item $y_O$ denotes an orientation node,
  \item $O$ denotes the available orientation. Orientations can be defined discretly and arbitrarily (e.g. 30, 60,...180 degrees) or they can be learned \cite{Nessler2010},
\end{itemize}

We redesign the network with a cascade of hierarchical WTA circuits. The input layer is a matrix of WTA circuits per spatial. Each input WTA circuit decides on the preferred orientation and intensity of its input. We will experiment with configuring the input WTA circuit to only relay intensity, only orientation, or both information. 

\section{Extending SEM for learning hidden features}

extending SEM by learning abstract features

We have seen the computational power of the SEM model as an unsupervised method for identifying hidden causes. So far the hidden causes have been used synonymously with predefined classes (e.g. digits\cite{LeCun1998}). We will extend the SEM model in a way that breaks this assumption. We insert an additional WTA circuit, responsible for learning hidden causes that depict abstract features of the object we're attempting to detect and recognize. This feature layer will contribute to the bottom-up learning as we expose it to the low-level input and and have it drive the WTA circuit already encountered in the original SEM architecture. With this additional feature-WTA circuit introduced we no longer require presentation of the entire stimulus but will restrict stimulus presentation to subregions within the space of a stimulus. These subregions may represent salient regions within a stimulus. The definition and method of selecting these subregions will be discussed in more detail as we discusss the object detection framework.

\chapter{Object detection}

\section{Attention}

Attention is the ability to economize computational power and reduce its entropy. 

\section{Attention mechanisms}

literature reivew of attention mechanisms

\section{Bottom-up Attention}

what we used from Itti's

\section{Top down attention}

describe attempt

\chapter{Achieving invariance}

\section{Model}

\section{Results}

\section{Discussion}

\section{Conclusion}

\appendix

\chapter{The First Appendix}

The \verb"\appendix" command should be used only once. Subsequent appendices can
be created using the Chapter command.

\chapter{The Second Appendix}

Some text for the second Appendix.

\bibliographystyle{plain}
\bibliography{collection}

\chapter{Afterword}

That's all folks!

The back matter often includes one or more of an index, an afterword,

\section{Acknowledgments}

Michael
Matthew Cook
My family: Sahra, father
My friends Malte Alf
INI
\end{document}
