Automatically generated by Mendeley 1.9.1
Any changes to this file will be lost if it is regenerated by Mendeley.

@article{Wiskott2004,
author = {Wiskott, Laurenz},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Wiskott - How Does Our Visual System Achieve Shift and Size Invariance Evidence for Shift and Size Invariance in the Visual System.pdf:pdf},
journal = {Sciences-New York},
number = {January 2001},
pages = {1--14},
title = {{How Does Our Visual System Achieve Shift and Size Invariance ? Evidence for Shift and Size Invariance in the Visual System}},
year = {2004}
}
@article{Serre,
author = {Serre, Thomas and Wolf, Lior and Poggio, Tomaso},
doi = {10.1109/CVPR.2005.254},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Serre, Wolf, Poggio - Object Recognition with Features Inspired by Visual Cortex.pdf:pdf},
isbn = {0-7695-2372-2},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
pages = {994--1000},
publisher = {Ieee},
title = {{Object Recognition with Features Inspired by Visual Cortex}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1467551},
volume = {2}
}
@article{Riesenhuber2000,
author = {Riesenhuber, Maximilian and Poggio, Tomaso},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Riesenhuber, Poggio - Models of object recognition.pdf:pdf},
journal = {America},
pages = {1199--1204},
title = {{Models of object recognition}},
year = {2000}
}
@article{Daugman1988,
author = {Daugman, John G},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review/Daugman - Complete Discrete 2-D Gabor Transforms by Neural Networks for Image Analysis and Compression.pdf:pdf},
journal = {IEEE Transactions On Acousticsm Speechm and Signal Processing},
number = {7},
pages = {1169--1179},
title = {{Complete Discrete 2-D Gabor Transforms by Neural Networks for Image Analysis and Compression}},
volume = {36},
year = {1988}
}
@article{Baluch2011,
abstract = {Attention exhibits characteristic neural signatures in brain regions that process sensory signals. An important area of future research is to understand the nature of top-down signals that facilitate attentional guidance towards behaviorally relevant locations and features. In this review, we discuss recent studies that have made progress towards understanding: (i) the brain structures and circuits involved in attentional allocation; (ii) top-down attention pathways, particularly as elucidated by microstimulation and lesion studies; (iii) top-down modulatory influences involving subcortical structures and reward systems; (iv) plausible substrates and embodiments of top-down signals; and (v) information processing and theoretical constraints that might be helpful in guiding future experiments. Understanding top-down attention is crucial for elucidating the mechanisms by which we can filter sensory information to pay attention to the most behaviorally relevant events.},
author = {Baluch, Farhan and Itti, Laurent},
doi = {10.1016/j.tins.2011.02.003},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review/Baluch, Itti - 2011 - Mechanisms of top-down attention.pdf:pdf},
issn = {1878-108X},
journal = {Trends in neurosciences},
keywords = {Animals,Attention,Attention: physiology,Brain,Brain Mapping,Brain: anatomy \& histology,Brain: physiology,Computer Simulation,Humans,Neural Pathways,Neural Pathways: anatomy \& histology,Visual Perception,Visual Perception: physiology},
month = apr,
number = {4},
pages = {210--24},
pmid = {21439656},
publisher = {Elsevier Ltd},
title = {{Mechanisms of top-down attention.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21439656},
volume = {34},
year = {2011}
}
@article{Nessler2010,
author = {Nessler, Bernhard and Pfeiffer, Michael and Maass, Wolfgang},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review/Nessler - STDP enables spiking neurons to detect hidden causes of their inputs.pdf:pdf},
journal = {In Proc. of NIPS 2009: Advances in Neural Information Processing Systems. MIT Press},
keywords = {Spike-based EM},
mendeley-tags = {Spike-based EM},
pages = {1357--1365},
title = {{STDP enables spiking neurons to detect hidden causes of their inputs}},
url = {http://www.igi.tugraz.at/maass/psfiles/191.pdf},
volume = {22},
year = {2010}
}
@article{Byrnes2010,
abstract = {A biologically inspired neuronal network that stores and recognizes temporal sequences of symbols is described. Each symbol is represented by excitatory input to distinct groups of neurons (symbol pools). Unambiguous storage of multiple sequences with common subsequences is ensured by partitioning each symbol pool into subpools that respond only when the current symbol has been preceded by a particular sequence of symbols. We describe synaptic structure and neural dynamics that permit the selective activation of subpools by the correct sequence. Symbols may have varying durations of the order of hundreds of milliseconds. Physiologically plausible plasticity mechanisms operate on a time scale of tens of milliseconds; an interaction of the excitatory input with periodic global inhibition bridges this gap so that neural events representing successive symbols occur on this much faster timescale. The network is shown to store multiple overlapping sequences of events. It is robust to variation in symbol duration, it is scalable, and its performance degrades gracefully with perturbation of its parameters.},
author = {Byrnes, Sean and Burkitt, Anthony N and Grayden, David B and Meffin, Hamish},
doi = {10.1162/neco.2009.12-07-679},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review/Byrnes et al. - 2010 - Spiking neuron model for temporal sequence recognition.pdf:pdf},
issn = {1530-888X},
journal = {Neural computation},
keywords = {Action Potentials,Action Potentials: physiology,Algorithms,Brain,Brain: physiology,Computer Simulation,Excitatory Postsynaptic Potentials,Excitatory Postsynaptic Potentials: physiology,Inhibitory Postsynaptic Potentials,Inhibitory Postsynaptic Potentials: physiology,Language,Mathematical Computing,Mathematical Concepts,Nerve Net,Nerve Net: physiology,Neural Inhibition,Neural Inhibition: physiology,Neural Networks (Computer),Neural Pathways,Neural Pathways: physiology,Neurons,Neurons: physiology,Reaction Time,Reaction Time: physiology,Speech Perception,Speech Perception: physiology,Symbolism,Synaptic Transmission,Synaptic Transmission: physiology,Time Factors,Time Perception,Time Perception: physiology},
month = jan,
number = {1},
pages = {61--93},
pmid = {19842991},
title = {{Spiking neuron model for temporal sequence recognition.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19842991},
volume = {22},
year = {2010}
}
@article{Serre2004,
author = {Serre, Thomas and Riesenhuber, Maximilian},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Serre, Riesenhuber - Realistic Modeling of Simple and Complex Cell Tuning in the HMAX Model , and Implications for Invariant Object Recognition in Cortex.pdf:pdf},
journal = {Methods},
number = {July},
title = {{Realistic Modeling of Simple and Complex Cell Tuning in the HMAX Model , and Implications for Invariant Object Recognition in Cortex}},
year = {2004}
}
@book{Grossberg2006,
author = {Grossberg, Stephen},
booktitle = {Computing},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Grossberg - TOWARDS A UNIFIED THEORY OF NEOCORTEX Laminar Cortical Circuits for Vision and Cognition.pdf:pdf},
isbn = {0001401106},
title = {{TOWARDS A UNIFIED THEORY OF NEOCORTEX : Laminar Cortical Circuits for Vision and Cognition}},
year = {2006}
}
@article{Riesenhuber1999,
abstract = {Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.},
author = {Riesenhuber, Maximilian and Poggio, Tomaso},
doi = {10.1038/14819},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Riesenhuber, Poggio - Hierarchical models of object recognition in cortex.pdf:pdf},
issn = {1097-6256},
journal = {Nature neuroscience},
keywords = {Animals,Computer Simulation,Form Perception,Form Perception: physiology,HMAX Model,Macaca,Mental Recall,Mental Recall: physiology,Models,Neurological,Neurons,Neurons: physiology,Visual Cortex,Visual Cortex: cytology,Visual Cortex: physiology,Visual Fields,Visual Fields: physiology,eines der am h\"{a}ufigsten zitierten biologischen Vis},
mendeley-tags = {eines der am h\"{a}ufigsten zitierten biologischen Vis,HMAX Model},
month = nov,
number = {11},
pages = {1019--25},
pmid = {10526343},
title = {{Hierarchical models of object recognition in cortex.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10526343},
volume = {2},
year = {1999}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G E and Salakhutdinov, R R},
doi = {10.1126/science.1127647},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Hinton, Salakhutdinov - Reducing the dimensionality of data with neural networks.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Deep Belief Networks},
mendeley-tags = {Deep Belief Networks},
month = jul,
number = {5786},
pages = {504--7},
pmid = {16873662},
title = {{Reducing the dimensionality of data with neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16873662},
volume = {313},
year = {2006}
}
@article{Nessler2013,
abstract = {The principles by which networks of neurons compute, and how spike-timing dependent plasticity (STDP) of synaptic weights generates and maintains their computational function, are unknown. Preceding work has shown that soft winner-take-all (WTA) circuits, where pyramidal neurons inhibit each other via interneurons, are a common motif of cortical microcircuits. We show through theoretical analysis and computer simulations that Bayesian computation is induced in these network motifs through STDP in combination with activity-dependent changes in the excitability of neurons. The fundamental components of this emergent Bayesian computation are priors that result from adaptation of neuronal excitability and implicit generative models for hidden causes that are created in the synaptic weights through STDP. In fact, a surprising result is that STDP is able to approximate a powerful principle for fitting such implicit generative models to high-dimensional spike inputs: Expectation Maximization. Our results suggest that the experimentally observed spontaneous activity and trial-to-trial variability of cortical neurons are essential features of their information processing capability, since their functional role is to represent probability distributions rather than static neural codes. Furthermore it suggests networks of Bayesian computation modules as a new model for distributed information processing in the cortex.},
author = {Nessler, Bernhard and Pfeiffer, Michael and Buesing, Lars and Maass, Wolfgang},
doi = {10.1371/journal.pcbi.1003037},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review/Nessler et al. - 2013 - Bayesian Computation Emerges in Generic Cortical Microcircuits through Spike-Timing-Dependent Plasticity.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
month = apr,
number = {4},
pages = {e1003037},
pmid = {23633941},
title = {{Bayesian Computation Emerges in Generic Cortical Microcircuits through Spike-Timing-Dependent Plasticity.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3636028\&tool=pmcentrez\&rendertype=abstract},
volume = {9},
year = {2013}
}
@article{Panchev2006,
abstract = {We present an approach for recognition and clustering of spatio tem- poral patterns based on networks of spiking neurons with active dendrites and dynamic synapses. We introduce a new model of an integrate-and- re neuron with active dendrites and dynamic synapses (ADDS) and its synaptic plasticity rule. The neuron employs the dynamics of the synapses and the active properties of the dendrites as an adaptive mechanism for maximizing its response to a speci c spatio-temporal distribution of in- coming action potentials. The learning algorithm follows recent biological evidence on synaptic plasticity. It goes beyond the current computational approaches which are based only on the relative timing between single pre- and post-synaptic spikes and implements a functional dependence based on the state of the dendritic and somatic membrane potentials around the pre- and post-synaptic action potentials. The learning algorithm is demonstrated to e ectively train the neuron towards a selective response determined by the spatio-temporal pattern of the onsets of input spike trains. The model is used in the implementation of a part of a robotic system for natural language instructions. We test the model with a robot whose goal is to recognize and execute language instructions. The research in this article demonstrates the potential of spiking neurons for processing spatio-temporal patterns and the experiments present spiking neural net- works as a paradigm which can be applied for modeling sequence detectors at word level for robot instructions},
author = {Panchev, Christo and Wermter, Stefan},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review/Panchev, Wermter - 2006 - Temporal Sequence Detection with Spiking Neurons Towards Recognizing Robot Language.pdf:pdf},
journal = {Connection Science},
keywords = {active dendrites,dynamic synapses,intelligent robotics,natural language,plasticity,spiking neurons,synaptic,temporal sequence detection},
number = {1},
pages = {1--22},
title = {{Temporal Sequence Detection with Spiking Neurons : Towards Recognizing Robot Language}},
url = {http://www.informatik.uni-hamburg.de/WTM/ps/panchevwermter05.pdf},
volume = {18},
year = {2006}
}
@article{Olshausen1993,
abstract = {We present a biologically plausible model of an attentional mechanism for forming position- and scale-invariant representations of objects in the visual world. The model relies on a set of control neurons to dynamically modify the synaptic strengths of intracortical connections so that information from a windowed region of primary visual cortex (V1) is selectively routed to higher cortical areas. Local spatial relationships (i.e., topography) within the attentional window are preserved as information is routed through the cortex. This enables attended objects to be represented in higher cortical areas within an object-centered reference frame that is position and scale invariant. We hypothesize that the pulvinar may provide the control signals for routing information through the cortex. The dynamics of the control neurons are governed by simple differential equations that could be realized by neurobiologically plausible circuits. In preattentive mode, the control neurons receive their input from a low-level "saliency map" representing potentially interesting regions of a scene. During the pattern recognition phase, control neurons are driven by the interaction between top-down (memory) and bottom-up (retinal input) sources. The model respects key neurophysiological, neuroanatomical, and psychophysical data relating to attention, and it makes a variety of experimentally testable predictions.},
author = {Olshausen, Bruno A. and Anderson, Charles H. and {Van Essen}, David C.},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Olshausen, Anderson, Van Essen - A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information.pdf:pdf},
issn = {0270-6474},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {Animals,Association Learning,Attention,Biological,Computer Simulation,Mathematics,Memory,Models,Neurons,Neurons: physiology,Pattern Recognition,Primates,Psychological,Visual,Visual Cortex,Visual Cortex: physiology,position-invariant vision},
mendeley-tags = {position-invariant vision},
month = nov,
number = {11},
pages = {4700--19},
pmid = {8229193},
title = {{A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8229193},
volume = {13},
year = {1993}
}
@article{Long2008,
author = {Long, Lyle N and Gupta, Ankur},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review/Long, Gupta - Biologically-Inspired Spiking Neural Networks with Hebbian Learning for Vision Processing.pdf:pdf},
journal = {Area},
pages = {1--17},
title = {{Biologically-Inspired Spiking Neural Networks with Hebbian Learning for Vision Processing}},
year = {2008}
}
@article{Serre2004a,
author = {Serre, Thomas and Wolf, Lior and Poggio, Tomaso},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Serre, Wolf, Poggio - A New Biologically Motivated Framework for Robust Object Recognition.pdf:pdf},
journal = {CBCL Paper/AI Memo},
number = {November},
title = {{A New Biologically Motivated Framework for Robust Object Recognition}},
volume = {2004},
year = {2004}
}
@article{LeCun1998,
author = {LeCun, Yann and Bottou, L\'{e}on and Bengio, Yoshua and Haffner, Patrick},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review/LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recognition.pdf:pdf},
journal = {Proceedings of the \ldots},
number = {11},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
url = {http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=726791},
volume = {86},
year = {1998}
}
@article{Itti2000,
abstract = {A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient},
author = {Itti, Laurent and Koch, Christof and Niebur, Ernst},
doi = {10.1016/S1053-5357(00)00088-3},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Itti, Koch, Niebur - Short papers from the April, 1998 Social Capital Conference at Michigan State University.pdf:pdf},
journal = {Journal of Socio-Economics},
keywords = {C++ implementation,Saliency maps,Visual Attention,good read},
mendeley-tags = {C++ implementation,good read,Saliency maps,Visual Attention},
month = nov,
number = {6},
pages = {579--586},
title = {{Short papers from the April, 1998 Social Capital Conference at Michigan State University}},
url = {http://cseweb.ucsd.edu/classes/fa09/cse258a/papers/itti-koch-1998.pdf},
volume = {29},
year = {2000}
}
@article{Walther2006,
author = {Walther, Dirk},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Walther - Interactions of Visual Attention and Object Recognition Computational Modeling , Algorithms , and Psychophysics Thesis by.pdf:pdf},
journal = {interactions},
title = {{Interactions of Visual Attention and Object Recognition : Computational Modeling , Algorithms , and Psychophysics Thesis by}},
volume = {2006},
year = {2006}
}
@article{Varga2011,
author = {Varga, Zsuzsanna and Jia, Hongbo and Sakmann, Bert and Konnerth, Arthur},
doi = {10.1073/pnas.1112355108},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Varga et al. - Dendritic coding of multiple sensory inputs in single cortical neurons in vivo.pdf:pdf},
pages = {6--11},
title = {{Dendritic coding of multiple sensory inputs in single cortical neurons in vivo}},
volume = {2011},
year = {2011}
}
@article{Habenschuss2013,
abstract = {The brain faces the problem of inferring reliable hidden causes from large populations of noisy neurons, for example, the direction of a moving object from spikes in area MT. It is known that a theoretically optimal likelihood decoding could be carried out by simple linear readout neurons if weights of synaptic connections were set to certain values that depend on the tuning functions of sensory neurons. We show here that such theoretically optimal readout weights emerge autonomously through STDP in conjunction with lateral inhibition between readout neurons. In particular, we identify a class of optimal STDP learning rules with homeostatic plasticity, for which the autonomous emergence of optimal readouts can be explained on the basis of a rigorous learning theory. This theory shows that the network motif we consider approximates expectation-maximization for creating internal generative models for hidden causes of high-dimensional spike inputs. Notably, we find that this optimal functionality can be well approximated by a variety of STDP rules beyond those predicted by theory. Furthermore, we show that this learning process is very stable and automatically adjusts weights to changes in the number of readout neurons, the tuning functions of sensory neurons, and the statistics of external stimuli.},
author = {Habenschuss, Stefan and Puhr, Helmut and Maass, Wolfgang},
doi = {10.1162/NECO\_a\_00446},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review/Habenschuss, Puhr, Maass - 2013 - Emergence of optimal decoding of population codes through STDP.pdf:pdf},
issn = {1530-888X},
journal = {Neural computation},
month = jun,
number = {6},
pages = {1371--407},
pmid = {23517096},
title = {{Emergence of optimal decoding of population codes through STDP.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23517096},
volume = {25},
year = {2013}
}
@article{Itti2001,
abstract = {Five important trends have emerged from recent work on computational models of focal visual attention that emphasize the bottom-up, image-based control of attentional deployment. First, the perceptual saliency of stimuli critically depends on the surrounding context. Second, a unique 'saliency map' that topographically encodes for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Third, inhibition of return, the process by which the currently attended location is prevented from being attended again, is a crucial element of attentional deployment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. And last, scene understanding and object recognition strongly constrain the selection of attended locations. Insights from these five key areas provide a framework for a computational and neurobiological understanding of visual attention.},
author = {Itti, Laurent and Koch, Christof},
doi = {10.1038/35058500},
file = {:C$\backslash$:/Users/woodstock/Documents/grad/Thesis/literature review//Itti, Koch - Computational modelling of visual attention.pdf:pdf},
issn = {1471-003X},
journal = {Nature reviews. Neuroscience},
keywords = {Animals,Attention,Attention: physiology,Computer Simulation,Humans,Models,Neurological,Neurons,Neurons: metabolism,Review,Visual Cortex,Visual Cortex: physiology,Visual Perception,Visual Perception: physiology},
mendeley-tags = {Review},
month = mar,
number = {3},
pages = {194--203},
pmid = {11256080},
title = {{Computational modelling of visual attention.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11256080},
volume = {2},
year = {2001}
}
